# 机器学习（Machine Leaning）

## 基本概念

什么是机器学习呢？简单来说就是找出一个解决问题的函数。

这里用语音识别（Speech Recognition）与图像识别（Image Recognition）举个例子：

- 对于语音识别，机器学习就是找到一个函数，将声音信号输出，转换为文字输出。
- 对于图像识别，机器学习就是找到一个函数，将图片作为输入，转换为文字输入。



那么机器学习又能够完成什么任务呢？

这里可以将其分为三类：

- **Regression（回归分析）**：例如根据房屋的各个属性（层高、面积、位置）来估算其房价。
- **Classification（分类）**：例如根据邮件内容分析该邮件是否为垃圾邮件。
- **Structured Leaning（结构化学习）**：如根据命令创造一个图片或一个文档。



在机器学习领域，我们常将机器学习任务理解为寻找一个能够解决特定问题的函数。那么，具体该如何找到这个函数呢？下面，我们以股票价格预测为例，深入探讨这一过程。 

假设我们的任务是基于某指定股票的当日价格，预测其明日价格。我们先假设描述这一关系的函数为：$y = b + wx_1$。

 在这个函数中，$x_1$代表当日股价，$y$ 代表预测的明日股价，而 $b$ 和 $w$ 是未知参数，分别被称作**偏置（bias）**与**权重（weight）** 。 

为了评估所选取的权重和偏置的优劣，我们需要定义一个**损失函数（Loss）**，记作$L(b, w)$。 

假定当前我们获取的偏置$b = 0.1$，权重$w = 0.97$，那么函数就确定为$y = 0.1 + 0.97x$。若选定某一天的股价$x_1 = 5.3$，通过该函数可得出预测结果$y = 5.241$。而当天股票实际价格为$\hat{y} = 5.33$，则预测误差为：$e_1 = |y - \hat{y}| = |5.241 - 5.33| = 0.089$。 

通常，损失函数$L = \frac{1}{N} \sum e_n$，它表示根据训练数据计算得到的所有误差值的平均值。上述计算误差时使用的公式$e_1 = |y - \hat{y}|$，被称作**平均绝对误差（MAE，Mean Absolute Error）**。除了MAE，另一种常见的误差计算公式是$e_1 = (y - \hat{y})^2$，即**均方误差（MSE，Mean Squared Error）**。 我们的目标是找出$b$和$w$的最优值，使损失函数$L$达到最小，从而让股价预测最为准确。寻找权重与偏置最优值的过程，被称为**梯度下降（Gradient Descent）**。

### 梯度下降（Gradient Descent）

梯度下降是一种广泛应用于优化问题的迭代算法，其核心原理是通过持续调整模型的权重（$w$）和偏置（$b$）的值，来寻找损失函数 $L$ 的最小值点。在机器学习中，损失函数用于衡量模型预测结果与真实标签之间的差异，我们的目标就是找到能使这个差异最小化的权重和偏置。

从数学角度来看，损失函数 $L$ 通常是关于权重 $w$ 和偏置 $b$ 的函数。在梯度下降算法里，我们沿着损失函数的负梯度方向逐步更新权重和偏置，因为负梯度方向是函数值下降最快的方向。每一次迭代时，权重和偏置的更新规则如下：

 $$w_{new} = w_{old} - \eta\frac{\partial L}{\partial w}$$ 

$$b_{new} = b_{old} - \eta\frac{\partial L}{\partial b}$$ 

其中，$\eta$ 是学习率（Learning Rate），它控制着每次更新的步长。学习率是一个非常关键的超参数，如果设置得过大，算法可能会跳过损失函数的最小值点，导致无法收敛；如果设置得过小，算法的收敛速度会变得非常缓慢，需要进行大量的迭代才能找到合适的权重和偏置。 

通过不断重复上述更新过程，权重和偏置会逐渐逼近最优值 $w^*$ 和 $b^*$，此时损失函数 $L$ 达到最小值，由这些最优权重和偏置所构成的函数，便是能有效解决我们既定问题的函数。  